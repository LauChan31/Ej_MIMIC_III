{"cells":[{"cell_type":"markdown","id":"senior-video","metadata":{"id":"senior-video"},"source":["# Pre-processing\n","In the previous script (extract_patients.ipynb), 17 clinical variables were extracted. Here we will do the preprocessing."]},{"cell_type":"code","execution_count":null,"id":"herbal-recipe","metadata":{"id":"herbal-recipe"},"outputs":[],"source":["# Imports:\n","import numpy as np\n","import pandas as pd\n","import os\n","import json"]},{"cell_type":"markdown","id":"secret-camera","metadata":{"id":"secret-camera"},"source":["### Read extracted data"]},{"cell_type":"code","execution_count":null,"id":"regular-supply","metadata":{"id":"regular-supply","outputId":"5dcf29a4-66f0-4d8b-ce07-537cc6eb7de0"},"outputs":[{"name":"stdout","output_type":"stream","text":["events_data (X):  (20567026, 14)\n","patients_data:  (30063, 25)\n","outcomes (Y):  (30063, 27)\n","Categorical:  ['Glascow coma scale eye opening', 'Glascow coma scale motor response', 'Glascow coma scale total', 'Glascow coma scale verbal response']\n","Continuous:  ['Diastolic blood pressure', 'Fraction inspired oxygen', 'Glucose', 'Heart Rate', 'Height', 'Mean blood pressure', 'Oxygen saturation', 'Respiratory rate', 'Systolic blood pressure', 'Temperature', 'Weight', 'pH']\n"]}],"source":["events_data = pd.read_hdf('vitals_hourly_data.h5', 'X')\n","events_data = events_data.reset_index()\n","print('events_data (X): ', events_data.shape)\n","\n","patients_data = pd.read_hdf('vitals_hourly_data.h5', 'patients_data')\n","print('patients_data: ', patients_data.shape)\n","\n","outcomes = pd.read_hdf('vitals_hourly_data.h5', 'Y')\n","print('outcomes (Y): ', outcomes.shape)\n","\n","# Load the config file that contains information about continuous/categorical variables:\n","config = json.load(open('resources/discretizer_config.json', 'r'))\n","is_categorical = config['is_categorical_channel']\n","\n","# Get categorical variables:\n","categorical_var = []\n","continuous_var = []\n","for key, value in is_categorical.items():\n","    if value:\n","        categorical_var.append(key)\n","    else:\n","        continuous_var.append(key)\n","print('Categorical: ', categorical_var[1:])\n","print('Continuous: ', continuous_var)\n","categorical_var = categorical_var[1:]"]},{"cell_type":"markdown","id":"helpful-illustration","metadata":{"id":"helpful-illustration"},"source":["### Pre-processing steps"]},{"cell_type":"code","execution_count":null,"id":"electronic-header","metadata":{"id":"electronic-header"},"outputs":[],"source":["# Map variables to the same metric:\n","UNIT_CONVERSIONS = [\n","    ('weight',                   'oz',  None,             lambda x: x/16.*0.45359237),\n","    ('weight',                   'lbs', None,             lambda x: x*0.45359237),\n","    ('fraction inspired oxygen', None,  lambda x: x > 1,  lambda x: x/100.),\n","    ('oxygen saturation',        None,  lambda x: x <= 1, lambda x: x*100.),\n","    ('temperature',              'f',   lambda x: x > 79, lambda x: (x - 32) * 5./9),\n","    ('height',                   'in',  None,             lambda x: x*2.54),\n","]\n","\n","variable_names = events_data['LEVEL1'].str\n","variable_units = events_data['valueuom'].str\n","for name, unit, check, convert_function in UNIT_CONVERSIONS:\n","    indices_variable = variable_names.contains(name, case=False, na=False)\n","    needs_conversion_filter_indices = indices_variable & False\n","    if unit is not None:\n","        needs_conversion_filter_indices |= variable_names.contains(unit, case=False, na=False) | variable_units.contains(unit, case=False, na=False)\n","    if check is not None:\n","        needs_conversion_filter_indices |= check(events_data['value'])\n","    idx = indices_variable & needs_conversion_filter_indices\n","    events_data.loc[idx, 'value'] = convert_function(events_data['value'][idx])"]},{"cell_type":"code","execution_count":null,"id":"impossible-adobe","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"impossible-adobe","executionInfo":{"status":"error","timestamp":1706998660072,"user_tz":300,"elapsed":10,"user":{"displayName":"Laura Sofia Chanchi Perez","userId":"17175016764690592694"}},"outputId":"f782e264-8f30-4cc5-9d67-06cb23bc40ad"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'pd' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-77046d71b3bb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0;31m# it is replaced with the nearest valid value.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvariable_ranges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'resources/variable_ranges.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvariable_ranges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LEVEL2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_ranges\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'LEVEL2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mvariable_ranges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_ranges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LEVEL2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"]}],"source":["# Detect and remove outliers. For this, they use two different outlier ranges:\n","# 1) for each variable, they have an upper and lower threshold for detecting unusable outliers.\n","#    If the outlier falls outside of these threshold, it is treated as missing.\n","# 2) they also have a physiologically valid range of measurements. If the non-outlier falls outside this range,\n","     # it is replaced with the nearest valid value.\n","\n","variable_ranges = pd.read_csv('resources/variable_ranges.csv', index_col = None)\n","variable_ranges['LEVEL2'] = variable_ranges['LEVEL2'].str.lower()\n","variable_ranges = variable_ranges.set_index('LEVEL2')\n","\n","variables_all = events_data['LEVEL2']\n","non_null_variables = ~events_data.value.isnull()\n","variables = set(variables_all)\n","range_names = set(variable_ranges.index.values)\n","range_names = [i.lower() for i in range_names]\n","\n","for var_name in variables:\n","    var_name_lower = var_name.lower()\n","\n","    if var_name_lower in range_names:\n","        out_low, out_high, val_low, val_high = [\n","            variable_ranges.loc[var_name_lower, x] for x in ('OUTLIER LOW', 'OUTLIER HIGH', 'VALID LOW', 'VALID HIGH')\n","        ]\n","\n","        # First find the indices of the variables that we need to check for outliers:\n","        indices_variable = non_null_variables & (variables_all == var_name)\n","\n","        # Check for low outliers and if they are not extreme, replace them with the imputation value:\n","        outlier_low_indices = (events_data.value < out_low)\n","        low_not_outliers = ~outlier_low_indices & (events_data.value < val_low)\n","        valid_low_indices = indices_variable & low_not_outliers\n","        events_data.loc[valid_low_indices, 'value'] = val_low\n","\n","        # Check for high outliers and if they are not extreme, replace them with the imputation value:\n","        outlier_high_indices = (events_data.value > out_high)\n","        high_not_outliers = ~outlier_high_indices & (events_data.value > val_high)\n","        valid_high_indices = indices_variable & high_not_outliers\n","        events_data.loc[valid_high_indices, 'value'] = val_high\n","\n","        # Treat values that are outside the outlier boundaries as missing:\n","        outlier_indices = indices_variable & (outlier_low_indices | outlier_high_indices)\n","        events_data.loc[outlier_indices, 'value'] = np.nan"]},{"cell_type":"markdown","id":"extensive-democrat","metadata":{"id":"extensive-democrat"},"source":["### Reshape data\n","We want to have a column for every variable:"]},{"cell_type":"code","execution_count":null,"id":"facial-flower","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"facial-flower","executionInfo":{"status":"error","timestamp":1707008431276,"user_tz":300,"elapsed":265,"user":{"displayName":"Laura Sofia Chanchi Perez","userId":"17175016764690592694"}},"outputId":"527d0eca-953a-44a8-b98c-2c29df3bfee0"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'events_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fbeb50d26163>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'icustay_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'itemid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEVEL1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEVEL2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mevents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'icustay_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'subject_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hadm_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LEVEL2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hours_in'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mevents_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'std'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mevents_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdroplevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mevents_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Aggregation Function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'events_data' is not defined"]}],"source":["events_data = events_data.set_index(['icustay_id', 'itemid', 'label', 'LEVEL1', 'LEVEL2'])\n","events_data = events_data.groupby(['icustay_id', 'subject_id', 'hadm_id', 'LEVEL2', 'hours_in'])\n","events_data = events_data.agg(['mean', 'std', 'count'])\n","events_data.columns = events_data.columns.droplevel(0)\n","events_data.columns.names = ['Aggregation Function']\n","events_data = events_data.unstack(level = 'LEVEL2')\n","events_data.columns = events_data.columns.reorder_levels(order=['LEVEL2', 'Aggregation Function'])"]},{"cell_type":"code","execution_count":null,"id":"abroad-fifteen","metadata":{"id":"abroad-fifteen"},"outputs":[],"source":["# Make sure we have a row for every hour:\n","missing_hours_fill = pd.DataFrame([[i, x] for i, y in patients_data['max_hours'].iteritems() for x in range(y+1)],\n","                                 columns=[patients_data.index.names[0], 'hours_in'])\n","missing_hours_fill['tmp'] = np.NaN\n","\n","fill_df = patients_data.reset_index()[['subject_id', 'hadm_id', 'icustay_id']].join(\n","     missing_hours_fill.set_index('icustay_id'), on='icustay_id')\n","fill_df.set_index(['icustay_id', 'subject_id', 'hadm_id', 'hours_in'], inplace=True)\n","\n","events_data = events_data.reindex(fill_df.index)\n","events_data = events_data.sort_index(axis = 0).sort_index(axis = 1)\n","\n","idx = pd.IndexSlice\n","events_data.loc[:, idx[:, 'count']] = events_data.loc[:, idx[:, 'count']].fillna(0)"]},{"cell_type":"code","execution_count":null,"id":"corrected-tuesday","metadata":{"id":"corrected-tuesday"},"outputs":[],"source":["# Save this version of the data as a .csv file, so we can apply different imputation methods in another notebook:\n","idx = pd.IndexSlice\n","timeseries_data = events_data.loc[:, idx[:, 'mean']]\n","timeseries_data = timeseries_data.droplevel('Aggregation Function', axis = 1)\n","timeseries_data = timeseries_data.reset_index()\n","timeseries_data.to_csv('mimic_timeseries_data_not_imputed.csv')"]},{"cell_type":"markdown","id":"quiet-chicken","metadata":{"id":"quiet-chicken"},"source":["### Imputation of time series data"]},{"cell_type":"code","execution_count":null,"id":"international-liberty","metadata":{"id":"international-liberty"},"outputs":[],"source":["idx = pd.IndexSlice\n","timeseries_data = events_data.loc[:, idx[:, ['mean', 'count']]]\n","\n","# Get the mean across hours for each variable and each patient:\n","icustay_means = timeseries_data.loc[:, idx[:, 'mean']].groupby(['subject_id', 'hadm_id', 'icustay_id']).mean()\n","\n","# Get the global mean for each variable:\n","global_means = timeseries_data.loc[:, idx[:, 'mean']].mean(axis = 0)\n","\n","# Forward fill the nan time series, or otherwise fill in the patient's mean or global mean:\n","timeseries_data.loc[:, idx[:, 'mean']] = timeseries_data.loc[:, idx[:, 'mean']].groupby(\n","    ['subject_id', 'hadm_id', 'icustay_id']).fillna(method='ffill').groupby(\n","    ['subject_id', 'hadm_id', 'icustay_id']).fillna(icustay_means).fillna(global_means)\n","\n","# Create a mask that indicates if the variable is present:\n","timeseries_data.loc[:, idx[:, 'count']] = (events_data.loc[:, idx[:, 'count']] > 0).astype(float)\n","timeseries_data.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n","\n","# Add a variable that indicates the time since the last measurement to the dataframe:\n","is_absent = (1 - timeseries_data.loc[:, idx[:, 'mask']])\n","hours_of_absence = is_absent.cumsum()\n","time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n","time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n","timeseries_data = pd.concat((timeseries_data, time_since_measured), axis = 1)\n","timeseries_data.loc[:, idx[:, 'time_since_measured']] = timeseries_data.loc[:, idx[:, 'time_since_measured']].fillna(100)\n","timeseries_data.sort_index(axis=1, inplace=True)"]},{"cell_type":"markdown","id":"copyrighted-challenge","metadata":{"id":"copyrighted-challenge"},"source":["### Standardization of continuous data"]},{"cell_type":"code","execution_count":null,"id":"allied-virgin","metadata":{"id":"allied-virgin"},"outputs":[],"source":["# Minmax standardization:\n","def minmax(x):\n","    mins = x.min()\n","    maxes = x.max()\n","    x_std = (x - mins) / (maxes - mins)\n","    return x_std\n","\n","def std_time_since_measurement(x):\n","    idx = pd.IndexSlice\n","    x = np.where(x==100, 0, x)\n","    means = x.mean()\n","    stds = x.std() + 0.0001\n","    x_std = (x - means)/stds\n","    return x_std\n","\n","timeseries_data.loc[:, idx[continuous_var, 'mean']] = timeseries_data.loc[:, idx[continuous_var, 'mean']].apply(lambda x: minmax(x))\n","timeseries_data.loc[:, idx[:, 'time_since_measured']] = timeseries_data.loc[:, idx[:, 'time_since_measured']].apply(lambda x: std_time_since_measurement(x))"]},{"cell_type":"markdown","id":"greek-swimming","metadata":{"id":"greek-swimming"},"source":["### One-hot encoding categorical variables"]},{"cell_type":"code","execution_count":null,"id":"agricultural-bracelet","metadata":{"id":"agricultural-bracelet"},"outputs":[],"source":["# First we need to round the categorical variables to the nearest category:\n","categorical_data = timeseries_data.loc[:, idx[categorical_var, 'mean']].copy(deep=True)\n","categorical_data = categorical_data.round()\n","one_hot = pd.get_dummies(categorical_data, columns=categorical_var)\n","\n","# Clean up the columns that we do not need and add the dummy encodings:\n","for c in categorical_var:\n","    if c in timeseries_data.columns:\n","        timeseries_data.drop(c, axis = 1, inplace=True)\n","timeseries_data.columns = timeseries_data.columns.droplevel(-1)\n","timeseries_data = pd.merge(timeseries_data.reset_index(), one_hot.reset_index(), how='inner', left_on=['subject_id', 'icustay_id', 'hadm_id', 'hours_in'],\n","                           right_on=['subject_id', 'icustay_id', 'hadm_id', 'hours_in'])\n","timeseries_data = timeseries_data.set_index(['subject_id', 'icustay_id', 'hadm_id', 'hours_in'])"]},{"cell_type":"markdown","id":"dependent-sister","metadata":{"id":"dependent-sister"},"source":["### Preprocessing of Y / outcomes"]},{"cell_type":"code","execution_count":null,"id":"cardiovascular-weekly","metadata":{"id":"cardiovascular-weekly"},"outputs":[],"source":["# First get the number of nan values per variable:\n","print(outcomes.isna().sum())\n","\n","# We will replace them with zero:\n","outcomes = outcomes.fillna(0)"]},{"cell_type":"markdown","id":"broken-arnold","metadata":{"id":"broken-arnold"},"source":["### Save all pre-processed data"]},{"cell_type":"code","execution_count":null,"id":"continuing-biology","metadata":{"id":"continuing-biology"},"outputs":[],"source":["# Rename the columns and save the results:\n","s = timeseries_data.columns.to_series()\n","timeseries_data.columns = s + s.groupby(s).cumcount().astype(str).replace({'0':''})\n","\n","timeseries_data.to_hdf('vitals_hourly_data_preprocessed.h5', 'X')\n","outcomes.to_hdf('vitals_hourly_data_preprocessed.h5', 'Y')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}